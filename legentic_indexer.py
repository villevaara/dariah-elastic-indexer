from smart_open import open
from elasticsearch import Elasticsearch, helpers
from glob import glob
import json
import ndjson
from datetime import datetime
from tqdm import tqdm
from lib.utils import read_elastic_pwd


def get_allas_url_ndjson(allas_url, add_id=True):
    with open(allas_url, encoding='utf-8') as rawfile:
        rawdata = rawfile.read()
        jsondata = json.loads(rawdata)
        docs = jsondata['documents']
        retdocs = list()
        for doc in docs:
            thisdoc = doc['fields']
            if add_id:
                thisdoc['_id'] = doc['fields']['url']
            retdocs.append(thisdoc)
    return retdocs




# testfile = 'https://a3s.fi/legentic-data/2016-01-01-2VSN8P.gz'

# for line in open(testfile, encoding='utf-8'):
#     print(repr(line))

allas_url = 'https://a3s.fi/'

# read filelist, append to ALLAS url
with open('../dariah-elastic-data/legentic_files.txt', 'r') as txtfile:
    allas_items = [allas_url + item.strip() for item in txtfile.readlines()]


# get all keys
all_keys = set()
for allas_url in tqdm(allas_items[:10000]):
    item = get_allas_url_ndjson(allas_url, add_id=False)
    for entry in item:
        all_keys.update(set(entry.keys()))


# find fields that are subfields.
# Create new mappings for the parent fields with the name [parent_field].content
is_subfield = list()
remappings = dict()
for key in all_keys:
    if "." in key:
        is_subfield.append(key)
        new_parent = ".".join(key.split(".")[:-1] + ['content'])
        old_parent = ".".join(key.split(".")[:-1])
        remappings[old_parent] = new_parent


with open("data/legentic_remappings.json", 'w') as jsonfile:
    json.dump(remappings, jsonfile)


# assign new eays for fields to be remapped
def remap_inputdata(inputdata, remapping):
    remapped_data = dict()
    for k, v in inputdata.items():
        if k in remapping.keys():
            remapped_data[remapping[k]] = v
        else:
            remapped_data[k] = v
    return remapped_data

def fix_version_field(input_item):
    # input_item['version'] = "_" + "-".join(input_item['version'])
    del input_item['version']

# testdoc = get_allas_url_ndjson(allas_items[0])[0]
# remapped_doc = remap_inputdata(testdoc, remappings)

# Password for the 'elastic' user generated by Elasticsearch
ELASTIC_PASSWORD = read_elastic_pwd("./secrets.txt")
# Create the client instance
client = Elasticsearch("https://ds-es.rahtiapp.fi:443",
                       basic_auth=("elastic", ELASTIC_PASSWORD), request_timeout=60)
index_name = "legentic"


mapping = {
    "properties": {
        'indexed': {"type": "date", "format": "yyyy-MM-dd HH:mm:ss Z"},
        'published': {"type": "date", "format": "yyyy-MM-dd HH:mm:ss Z"},
        'author.name': {"type": "text"},
        'author.text.hashtag': {"type": "keyword"},
        'author.text.user_mention': {"type": "keyword"},
        'author.text.content': {"type": "keyword"},
        'author.content': {"type": "keyword"},
        'author-uri': {"type": "keyword"},
        'author-community_facebook': {"type": "keyword"},
        'author-id_facebook': {"type": "keyword"},
        'author-following': {"type": "integer"},
        'author-followers': {"type": "integer"},
        'blog_id': {"type": "keyword"},
        'citation.length': {"type": "integer"},
        'citation.content': {"type": "keyword"},
        'description.content': {"type": "text"},
        'facebook_id': {"type": "keyword"},
        'forum_post_id': {"type": "keyword"},
        'google-id': {"type": "keyword"},
        'instagram-author-id': {"type": "keyword"},
        'instagram-ref-author-id': {"type": "keyword"},
        'language': {"type": "keyword"},
        'latency': {"type": "long"},
        'latitude': {"type": "float"},
        'longitude': {"type": "float"},
        'name.content': {"type": "text"},
        'page-title.content': {"type": "text"},
        'quote.content': {"type": "text"},
        'type': {"type": "keyword"},
        'url': {"type": "keyword"},
        'ref-author': {"type": "text"},
        'ref-author-id_facebook': {"type": "keyword"},
        'ref-author-uri': {"type": "keyword"},
        'subject.content': {"type": "keyword"},
        'subject.length': {"type": "integer"},
        'text.address.size': {"type": "integer"},
        'text.content': {"type": "text"},
        'text.email.size': {"type": "integer"},
        'text.hashtag': {"type": "keyword"},
        'text.person.content': {"type": "keyword"},
        'text.person.size': {"type": "integer"},
        'text.phone.canonized': {"type": "keyword"},
        'text.phone.content': {"type": "text"},
        'text.url.content': {"type": "keyword"},
        'text.url.length': {"type": "integer"},
        'text.user_mention': {"type": "keyword"},
        'thread.title.content': {"type": "text"},
        'twitter_retweet_id': {"type": "keyword"},
        'twitter_tweet_id': {"type": "keyword"},
        'youtube-channel-id': {"type": "keyword"},
        # 'version': {"type": "keyword"},
    }
}


# deleting an index
client.indices.delete(index=index_name)

# create the index if it doesn't exist
client.indices.create(index=index_name, mappings=mapping)


# read data

# index doc range, one document at a time - for testing, this gives more detailed error messages than the bulk API
# test_data = read_ndjson(data_json[859])
# for i in range(0, 1000):
#     doc = test_data[i]
#     resp = client.index(index=index_name, id=doc["speech_id"].replace("_", "-"), document=doc)
#     print(str(i) + " " + resp['result'])

testdata = get_allas_url_ndjson(allas_items[0], add_id=False)
doc = testdata[0]
doc_remapped = remap_inputdata(doc, remappings)
resp = client.index(index=index_name, id=doc_remapped["url"], document=doc_remapped)


def remap_bulk_batch(items_batch, remappings, fix_version=True):
    remapped = list()
    for item in items_batch:
        new_item = remap_inputdata(item, remappings)
        if fix_version:
            fix_version_field(new_item)
        remapped.append(new_item)
    return remapped


# index bulk - updates if id already present.
i = 0
for item in tqdm(allas_items):
    print(str(i) + " - " + item)
    inputdata = get_allas_url_ndjson(allas_items[0])
    input_remapped = remap_bulk_batch(inputdata, remappings)
    helpers.bulk(client, input_remapped, index=index_name)
    i += 1


# one at a time
# index doc range, one document at a time - for testing, this gives more detailed error messages than the bulk API
for bulk_batch in allas_items:
    print(bulk_batch)
    testdata = get_allas_url_ndjson(bulk_batch, add_id=False)
    test_remapped = remap_bulk_batch(testdata, remappings)
    for item in tqdm(test_remapped):
        client.index(index=index_name, id=item["url"], document=item)


# test_data = read_ndjson(data_json[859])
# for i in range(0, 1000):
#     doc = test_data[i]
#     resp = client.index(index=index_name, id=doc["speech_id"].replace("_", "-"), document=doc)
#     print(str(i) + " " + resp['result'])

