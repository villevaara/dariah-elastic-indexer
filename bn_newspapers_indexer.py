from smart_open import open
from elasticsearch import Elasticsearch, helpers
import json
from lib.utils import read_elastic_pwd, log_line, read_indexed_log, get_id_from_str
from glob import glob
import csv
from tqdm import tqdm


def setup_inputdata(input_item, meta_dict):
    outputdata = dict()
    ok = {
        "issueID": "issue_id",
        "articleType": "article_type",
        "articleID": "article_id",
        "newspaperID": "newspaper_id",
        "text": "text",
        "title": "title",
        "paragraphs": "paragraphs"}
    for k, v in ok.items() :
        if k in input_item.keys():
            outputdata[v] = input_item[k]
    this_meta = meta_dict[input_item["articleID"]]
    for meta_key in ["newspaper_title",
        "vol_no",
        "issue_no",
        "thematic_collection",
        "octavo_collection",
        "issue_date_start",
        "issue_date_end",
        "original_issue_date"]:
        outputdata[meta_key] = this_meta[meta_key]
    for key in ["issue_date_start", "issue_date_end"]:
        outputdata[key] = outputdata[key].replace('-00', '-01')
        outputdata[key] = outputdata[key].replace('-02-29', '-02-28')
    outputdata["_id"] = get_id_from_str(input_item["articleID"])
    return outputdata


# read dictionary to remap field names. Some caused trouble in indexing.
# see legentic_indexer.py for how the remappings were done.
with open('data/legentic_remappings.json', 'r') as jsonfile:
    remappings = json.load(jsonfile)

# Password for the 'elastic' user generated by Elasticsearch
ELASTIC_PASSWORD = read_elastic_pwd("./secrets.txt")
# Create the client instance
# test env client
client = Elasticsearch("https://dariahfi-es.2.rahtiapp.fi:443",
                       basic_auth=("elastic", ELASTIC_PASSWORD), request_timeout=60)
index_name = "bn"

mapping = {
    "properties": {
        "issue_id": {"type": "keyword"},
        "article_type": {"type": "keyword"},
        "article_id": {"type": "keyword"},
        "newspaper_id": {"type": "keyword"},
        "text": {"type": "text"},
        "title": {"type": "text"},
        "paragraphs": {"type": "integer"},
        "newspaper_title": {"type": "keyword"},
        "vol_no": {"type": "text"},
        "issue_no": {"type": "text"},
        "thematic_collection": {"type": "keyword"},
        "octavo_collection": {"type": "keyword"},
        "issue_date_start": {"type": "date", "format": "yyyy-MM-dd"},
        "issue_date_end": {"type": "date", "format": "yyyy-MM-dd"},
        "original_issue_date": {"type": "text"}
    }
}

index_settings = {
    'number_of_shards': 10,
    'codec': 'best_compression'
}


logf_path = "logs/bn_indexed.log"
already_indexed = read_indexed_log(logf_path)

# Get arguments for start and end index
# parser = argparse.ArgumentParser(description='Optional: Input start and end iterations.')
# parser.add_argument('--reindex', dest='reindex', action='store_true', help='Do not skip already indexed.')
# parser.add_argument('--no-reindex', dest='reindex', action='store_false', help='Skip indexed. Default.')
# parser.set_defaults(reindex=False)
#
# args = parser.parse_args()
# reindex = args.reindex
reindex = False


meta_dict = {}
with open("../dariah-elastic-data/bn/bl_newspapers_meta.csv", 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        meta_dict[row['article_id']] = row


all_items = glob("../dariah-elastic-data/bn/json_res/*.json")

for this_item in tqdm(all_items):
    with open(this_item, 'r') as jsonfile:
        if this_item not in already_indexed or reindex:
            inputdata = json.load(jsonfile)
            inputdata_final = list()
            for item in inputdata:
                inputdata_final.append(setup_inputdata(item, meta_dict))
            helpers.bulk(client, inputdata_final, index=index_name)
            log_line(logfile=logf_path, line=this_item)


# index single doc
# test_data = read_ndjson(data_json[0])
# doc = test_data[0]
# resp = client.index(index=index_name, document=doc)
# print(resp['result'])

